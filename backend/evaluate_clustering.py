"""
Clustering Evaluation Module

Evaluates clustering quality using multiple metrics:
- Silhouette Score: measures cluster cohesion and separation (-1 to 1, higher is better)
- Davies-Bouldin Index: average similarity ratio (lower is better)
- Calinski-Harabasz Index: ratio of between-cluster to within-cluster dispersion (higher is better)

Also provides:
- Per-cluster statistics (size, gender balance, diversity breakdown)
- Comparison tables
- Visualizations for report/presentation
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, Tuple, Optional
from sklearn.metrics import (silhouette_score, davies_bouldin_score, calinski_harabasz_score, pairwise_distances)


def evaluate_gower_kmedoids(
    df: pd.DataFrame, 
    feature_matrix: np.ndarray, 
    n_clusters: int, 
    random_state: int,
    compute_distance_matrix_func, # Pass the function to avoid circular imports
    kmedoids_pam_from_matrix_func
):
    """
    Performs K-Medoids clustering using the Gower distance matrix and evaluates results.
    """
    print("\n=== Running K-Medoids with Gower Distance ===")
    
    # 1. Compute all distance matrices (including Gower)
    _, _, D_gower = compute_distance_matrix_func(df, feature_matrix)
    
    # 2. Run K-Medoids (PAM) using the Gower distance matrix
    labels, _ = kmedoids_pam_from_matrix_func(D_gower, n_clusters, random_state=random_state)
    
    # 3. Evaluate results using the general function (Option A)
    # This uses the labels generated by Gower, but the distance metrics (DB, CH, Silhouette)
    # rely on the Euclidean feature_matrix for compatibility.
    gower_eval_results = evaluate_clustering(
        feature_matrix, 
        labels, 
        df, 
        metric_name="Gower", 
        algorithm_name="K-Medoids"
    )
    
    # 4. CRUCIAL: Override Silhouette Score with the Gower-based distance matrix
    # This provides the true measure of Gower's effectiveness.
    silhouette_gower = silhouette_score(D_gower, labels, metric='precomputed')
    gower_eval_results['silhouette_score'] = silhouette_gower
    
    print(f"Gower Silhouette Score (True): {silhouette_gower:.4f}")

    return gower_eval_results



def evaluate_clustering(
    feature_matrix: np.ndarray,
    labels: np.ndarray,
    df: pd.DataFrame,
    metric_name: str = "Euclidean",
    algorithm_name: str = "K-Means"
):
    """
    Compute all clustering quality metrics for a given clustering result.

    Args:
        feature_matrix: NxM feature matrix
        labels: Cluster labels for each sample
        df: Original DataFrame (for per-cluster statistics)
        metric_name: Name of distance metric used (e.g., "Euclidean", "Manhattan")
        algorithm_name: Name of algorithm used (e.g., "K-Means", "K-Medoids")

    Returns:
        Dictionary with computed metrics and statistics
    """
    n_clusters = len(np.unique(labels))
    n_samples = len(labels)

    # Clustering quality metrics
    silhouette = silhouette_score(feature_matrix, labels)
    davies_bouldin = davies_bouldin_score(feature_matrix, labels)
    calinski_harabasz = calinski_harabasz_score(feature_matrix, labels)

    G_entropy = []

    # Per-cluster statistics
    cluster_stats = []
    for cluster_id in sorted(np.unique(labels)):
        cluster_mask = labels == cluster_id
        cluster_size = cluster_mask.sum()
        cluster_df = df[cluster_mask]

        stats = {
            "cluster_id": int(cluster_id) + 1,  # 1-indexed for display
            "size": int(cluster_size),
        }

        # Gender breakdown
        if "Gender" in cluster_df.columns:
            gender_counts = cluster_df["Gender"].value_counts().to_dict()
            stats["gender_breakdown"] = gender_counts
            # Calculate gender balance (entropy, lower = more balanced)
            gender_props = np.array(list(gender_counts.values())) / cluster_size
            gender_entropy = -np.sum(gender_props * np.log(gender_props + 1e-10))
            stats["gender_entropy"] = gender_entropy
            G_entropy.append(gender_entropy)
        else:
            stats["gender_breakdown"] = {}
            stats["gender_entropy"] = 0.0
            G_entropy.append(0.0)

        # Diversity breakdown
        if "Diversity" in cluster_df.columns:
            diversity_counts = cluster_df["Diversity"].value_counts().to_dict()
            stats["diversity_breakdown"] = diversity_counts
            # Diversity balance (entropy)
            diversity_props = np.array(list(diversity_counts.values())) / cluster_size
            diversity_entropy = -np.sum(diversity_props * np.log(diversity_props + 1e-10))
            stats["diversity_entropy"] = diversity_entropy
        else:
            stats["diversity_breakdown"] = {}
            stats["diversity_entropy"] = 0.0

        cluster_stats.append(stats)
    
    mean_G_entropy = np.mean(G_entropy) if G_entropy else 0.0

    return {
        "algorithm": algorithm_name,
        "metric": metric_name,
        "n_clusters": n_clusters,
        "n_samples": n_samples,
        "silhouette_score": silhouette,
        "davies_bouldin_index": davies_bouldin,
        "calinski_harabasz_index": calinski_harabasz,
        "mean_G_entropy": mean_G_entropy,
        "cluster_stats": cluster_stats,
    }


def print_evaluation_report(eval_result: Dict) -> None:
    """
    Pretty-print a clustering evaluation report with all metrics and per-cluster stats.
    """
    print("\n" + "="*80)
    print(f"CLUSTERING EVALUATION REPORT")
    print("="*80)
    print(f"Algorithm: {eval_result['algorithm']} | Metric: {eval_result['metric']}")
    print(f"Number of Clusters: {eval_result['n_clusters']} | Total Samples: {eval_result['n_samples']}")
    print()

    print("CLUSTERING QUALITY METRICS")
    print("-" * 80)
    print(f"  Silhouette Score (↑ better, -1 to 1):        {eval_result['silhouette_score']:>8.4f}")
    print(f"  Davies-Bouldin Index (↓ better):             {eval_result['davies_bouldin_index']:>8.4f}")
    print(f"  Calinski-Harabasz Index (↑ better):          {eval_result['calinski_harabasz_index']:>8.2f}")
    print(f" Mean Gender Entropy (↑ better):               {eval_result['mean_G_entropy']:>8.4f}")
    print()


    print("PER-CLUSTER STATISTICS")
    print("-" * 80)
    for stat in eval_result["cluster_stats"]:
        print(f"\nCluster {stat['cluster_id']} ({stat['size']} students):")
        if stat["gender_breakdown"]:
            print(f"  Gender Balance:    {stat['gender_breakdown']} (entropy: {stat['gender_entropy']:.3f})")
        if stat["diversity_breakdown"]:
            print(f"  Diversity:         {stat['diversity_breakdown']}")
    print()


def create_comparison_table(results: list) -> pd.DataFrame:
    """
    Create a comparison table from multiple evaluation results.

    Args:
        results: List of dictionaries returned from evaluate_clustering()

    Returns:
        DataFrame with metrics for easy comparison
    """
    rows = []
    for res in results:
        rows.append({
            "Algorithm": res["algorithm"],
            "Metric": res["metric"],
            "N Clusters": res["n_clusters"],
            "Silhouette Score": f"{res['silhouette_score']:.4f}",
            "Davies-Bouldin": f"{res['davies_bouldin_index']:.4f}",
            "Calinski-Harabasz": f"{res['calinski_harabasz_index']:.2f}",
            "Gender Balance (Mean Entropy)": f"{res.get('mean_G_entropy', 0.0):.4f}",
        })

    df = pd.DataFrame(rows)
    return df


def plot_metrics_comparison(results: list, save_path: Optional[str] = None):
    """
    Visualize clustering metrics comparison across different algorithms/metrics.

    Args:
        results: List of evaluation results
        save_path: Optional path to save the figure
    """
    names = [f"{r['algorithm']}\n({r['metric']})" for r in results]
    silhouette_scores = [r['silhouette_score'] for r in results]
    davies_bouldin_scores = [r['davies_bouldin_index'] for r in results]
    ch_scores = [r['calinski_harabasz_index'] for r in results]

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # Silhouette Score (higher is better)
    ax = axes[0]
    bars = ax.bar(names, silhouette_scores, color=['green' if s > 0.5 else 'orange' if s > 0 else 'red' for s in silhouette_scores])
    ax.set_ylabel('Score', fontsize=11)
    ax.set_title('Silhouette Score (↑ Higher is Better)', fontsize=12, fontweight='bold')
    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    ax.set_ylim([min(-0.1, min(silhouette_scores) - 0.1), max(1.0, max(silhouette_scores) + 0.1)])
    for bar, score in zip(bars, silhouette_scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02, f'{score:.3f}',
                ha='center', va='bottom', fontsize=9)

    # Davies-Bouldin Index (lower is better)
    ax = axes[1]
    bars = ax.bar(names, davies_bouldin_scores, color=['green' if s < 1.5 else 'orange' if s < 2.0 else 'red' for s in davies_bouldin_scores])
    ax.set_ylabel('Index', fontsize=11)
    ax.set_title('Davies-Bouldin Index (↓ Lower is Better)', fontsize=12, fontweight='bold')
    for bar, score in zip(bars, davies_bouldin_scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05, f'{score:.3f}',
                ha='center', va='bottom', fontsize=9)


    ax = axes[2]
    bars = ax.bar(names, ch_scores, color='steelblue')
    ax.set_ylabel('Index', fontsize=11)
    ax.set_title('Calinski-Harabasz Index (↑ Higher is Better)', fontsize=12, fontweight='bold')
    for bar, score in zip(bars, ch_scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05, f'{score:.1f}',
                ha='center', va='bottom', fontsize=9)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"\nMetrics comparison plot saved to: {save_path}")

    plt.show()


def plot_cluster_balance(results: list, save_path: Optional[str] = None) -> None:
    """
    Visualize per-cluster gender and diversity balance across results.

    Args:
        results: List of evaluation results
        save_path: Optional path to save the figure
    """
    fig, axes = plt.subplots(len(results), 1, figsize=(10, 3 * len(results)))
    if len(results) == 1:
        axes = [axes]

    for idx, result in enumerate(results):
        ax = axes[idx]
        title = f"{result['algorithm']} ({result['metric']})"

        cluster_ids = [s['cluster_id'] for s in result['cluster_stats']]
        gender_entropies = [s['gender_entropy'] for s in result['cluster_stats']]
        diversity_entropies = [s['diversity_entropy'] for s in result['cluster_stats']]

        x = np.arange(len(cluster_ids))
        width = 0.35

        bars1 = ax.bar(x - width/2, gender_entropies, width, label='Gender Entropy', alpha=0.8, color='steelblue')
        bars2 = ax.bar(x + width/2, diversity_entropies, width, label='Diversity Entropy', alpha=0.8, color='coral')

        ax.set_xlabel('Cluster ID', fontsize=10)
        ax.set_ylabel('Entropy (↓ More Balanced)', fontsize=10)
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels([f"G{cid}" for cid in cluster_ids])
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Cluster balance plot saved to: {save_path}")

    plt.show()
